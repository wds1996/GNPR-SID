{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from openlocationcode import openlocationcode as olc\n",
    "\n",
    "# Filter out users and POIs with low frequency\n",
    "def do_filter(df, poi_min_freq=10, user_min_freq=10):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # df['PoiFreq'] = df.groupby('Pid')['Uid'].transform('nunique')\n",
    "    df['PoiFreq'] = df.groupby('Pid')['Uid'].transform('count')\n",
    "    df = df[df['PoiFreq'] >= poi_min_freq]\n",
    "    \n",
    "    # df['UserFreq'] = df.groupby('Uid')['Pid'].transform('nunique')\n",
    "    df['UserFreq'] = df.groupby('Uid')['Pid'].transform('count')\n",
    "    df = df[df['UserFreq'] >= user_min_freq]\n",
    "\n",
    "    df = df.drop(columns=['PoiFreq', 'UserFreq'])\n",
    "    \n",
    "    # df = df.groupby('Pid').filter(lambda x: len(x) > poi_min_freq)\n",
    "    # df = df.groupby('Uid').filter(lambda x: len(x) > user_min_freq)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_pluscode(latitude, longitude):\n",
    "    # Plus Code\n",
    "    plus_code = olc.encode(latitude, longitude)\n",
    "    return plus_code[:6]\n",
    "\n",
    "file_name = \"datasets/NYC.txt\"\n",
    "df = pd.read_csv(file_name, sep=\"\\t\", encoding='latin-1', header=None, names=[\n",
    "    \"User ID\", \"Venue ID\", \"Venue Category ID\", \"Venue Category Name\", \"Latitude\", \"Longitude\", \"Timezone Offset\", \"UTC Time\"\n",
    "])\n",
    "\n",
    "df[\"Region\"] = df.apply(lambda row: get_pluscode(row['Latitude'], row['Longitude']), axis=1)\n",
    "\n",
    "df[\"UTC Time\"] = pd.to_datetime(df[\"UTC Time\"], format=\"%a %b %d %H:%M:%S %z %Y\")\n",
    "\n",
    "df[\"Local Time\"] = (df[\"UTC Time\"] + df[\"Timezone Offset\"].apply(lambda x: timedelta(minutes=x))).dt.strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "\n",
    "df.columns = [\"Uid\", \"Pid\", \"Venue Category ID\", \"Catname\", \"Lat\", \"Lon\", \"Timezone Offset\", \"UTC Time\", \"Region\", \"Time\"]\n",
    "\n",
    "df = df[[\"Uid\", \"Pid\", \"Catname\", \"Region\", \"Time\"]]\n",
    "\n",
    "filtered_df = do_filter(df, poi_min_freq=10, user_min_freq=10)\n",
    "\n",
    "datapath = file_name.split(\".\")[0] + \"/\"\n",
    "outname = file_name.split(\"/\")[-1].split(\".\")[0]\n",
    "filtered_df.to_csv(f\"{datapath}{outname}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "dataset = \"NYC\"\n",
    "df = pd.read_csv(f\"datasets/{dataset}/{dataset}.csv\")\n",
    "\n",
    "uids = list(df[\"Uid\"].unique())\n",
    "pids = list(df[\"Pid\"].unique())\n",
    "cats = list(df[\"Catname\"].unique())\n",
    "regs = list(df[\"Region\"].unique())\n",
    "\n",
    "random.shuffle(uids)\n",
    "random.shuffle(pids)\n",
    "random.shuffle(cats)\n",
    "random.shuffle(regs)\n",
    "\n",
    "uid_map = {uid: i for i, uid in enumerate(uids, start=1)}\n",
    "pid_map = {pid: i for i, pid in enumerate(pids, start=1)}\n",
    "cat_map = {cat: i for i, cat in enumerate(cats, start=1)}\n",
    "reg_map = {reg: i for i, reg in enumerate(regs, start=1)}\n",
    "\n",
    "df[\"Uid\"] = df[\"Uid\"].map(uid_map)\n",
    "df[\"Pid\"] = df[\"Pid\"].map(pid_map)\n",
    "df[\"Catname\"] = df[\"Catname\"].map(cat_map)\n",
    "df[\"Region\"] = df[\"Region\"].map(reg_map)\n",
    "\n",
    "if not os.path.exists(f\"datasets/{dataset}\"):\n",
    "    os.makedirs(f\"datasets/{dataset}\")\n",
    "\n",
    "pd.DataFrame(list(uid_map.items()), columns=[\"Original_Uid\", \"Mapped_Uid\"]).to_csv(f\"datasets/{dataset}/uid_mapping.csv\", index=False)\n",
    "pd.DataFrame(list(pid_map.items()), columns=[\"Original_Pid\", \"Mapped_Pid\"]).to_csv(f\"datasets/{dataset}/pid_mapping.csv\", index=False)\n",
    "pd.DataFrame(list(cat_map.items()), columns=[\"Original_Catname\", \"Mapped_Catname\"]).to_csv(f\"datasets/{dataset}/catname_mapping.csv\", index=False)\n",
    "pd.DataFrame(list(reg_map.items()), columns=[\"Original_Region\", \"Mapped_Region\"]).to_csv(f\"datasets/{dataset}/region_mapping.csv\", index=False)\n",
    "\n",
    "\n",
    "df.to_csv(f\"datasets/{dataset}/data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import networkx as nx\n",
    "from datetime import datetime\n",
    "\n",
    "dataset = 'NYC'\n",
    "file_name = f\"datasets/{dataset}/data.csv\"\n",
    "\n",
    "df = pd.read_csv(file_name)\n",
    "df[\"Time\"] = pd.to_datetime(df[\"Time\"]).dt.hour\n",
    "\n",
    "\n",
    "\n",
    "poi_sequence = df.groupby(\"Uid\").agg({\n",
    "    \"Pid\": list,\n",
    "    \"Catname\": list\n",
    "}).reset_index()\n",
    "\n",
    "\n",
    "def get_forward_neighbors(df, column, min_freq=1):\n",
    "    neighbor_counts = defaultdict(Counter)\n",
    "    all_pois = set()\n",
    "\n",
    "    for sequence in df[column]:\n",
    "        all_pois.update(sequence)\n",
    "        for i in range(len(sequence) - 1):\n",
    "            current_poi = sequence[i]\n",
    "            next_poi = sequence[i + 1]\n",
    "            neighbor_counts[current_poi][next_poi] += 1\n",
    "\n",
    "    df_data = []\n",
    "    for poi in all_pois:\n",
    "        counter = neighbor_counts.get(poi, {})\n",
    "        filtered_neighbors = {\n",
    "            neighbor: freq for neighbor, freq in counter.items() if freq >= min_freq\n",
    "        }\n",
    "        if filtered_neighbors:\n",
    "            sorted_neighbors = [\n",
    "                neighbor for neighbor, _ in sorted(filtered_neighbors.items(), key=lambda x: x[1], reverse=True)\n",
    "            ]\n",
    "        else:\n",
    "            sorted_neighbors = []\n",
    "        df_data.append((poi, sorted_neighbors))\n",
    "\n",
    "    neighbors_df = pd.DataFrame(df_data, columns=[column, \"neighbors\"])\n",
    "    return neighbors_df\n",
    "\n",
    "def get_neighbors(df, column, min_freq=1):\n",
    "    neighbor_counts = defaultdict(Counter)\n",
    "    all_pois = set()\n",
    "\n",
    "    for sequence in df[column]:\n",
    "        all_pois.update(sequence)\n",
    "        for i, poi in enumerate(sequence):\n",
    "            if i > 0:  \n",
    "                neighbor_counts[poi][sequence[i - 1]] += 1\n",
    "            if i < len(sequence) - 1:  \n",
    "                neighbor_counts[poi][sequence[i + 1]] += 1\n",
    "\n",
    "    df_data = []\n",
    "    for poi in all_pois:\n",
    "        counter = neighbor_counts.get(poi, {})\n",
    "\n",
    "        filtered_neighbors = {\n",
    "            neighbor: freq for neighbor, freq in counter.items() if freq >= min_freq\n",
    "        }\n",
    "\n",
    "        sorted_neighbors = [\n",
    "            neighbor for neighbor, _ in sorted(filtered_neighbors.items(), key=lambda x: x[1], reverse=True)\n",
    "        ]\n",
    "        df_data.append((poi, sorted_neighbors))\n",
    "\n",
    "    neighbors_df = pd.DataFrame(df_data, columns=[column, \"neighbors\"])\n",
    "    return neighbors_df\n",
    "\n",
    "poi_info = df.groupby(\"Pid\").agg({\n",
    "    \"Uid\": list,\n",
    "    \"Catname\": lambda x: x.iloc[0],\n",
    "    \"Region\": lambda x: x.iloc[0],\n",
    "    \"Time\": list\n",
    "}).reset_index()\n",
    "\n",
    "poi_info[\"Uid\"] = poi_info[\"Uid\"].apply(lambda uids: [uid for uid, count in Counter(uids).items() if count >= 1])\n",
    "\n",
    "poi_info[\"Time\"] = poi_info[\"Time\"].apply(lambda times: [time for time, count in Counter(times).items() if count >= 1])\n",
    "\n",
    "poi_neighbors = get_neighbors(poi_sequence,\"Pid\", 1)\n",
    "poi_info[\"neighbors\"] = poi_info[\"Pid\"].map(poi_neighbors.set_index(\"Pid\")[\"neighbors\"])\n",
    "\n",
    "forward_neighbors = get_forward_neighbors(poi_sequence,\"Pid\", 1)\n",
    "poi_info[\"forward_neighbors\"] = poi_info[\"Pid\"].map(forward_neighbors.set_index(\"Pid\")[\"neighbors\"])\n",
    "\n",
    "\n",
    "poi_info.to_csv(f\"datasets/{dataset}/poi_info.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = \"NYC\"\n",
    "file_name = f\"datasets/{dataset}/data.csv\"\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "df = df[['Uid', 'Pid', 'Time']]\n",
    "\n",
    "df = df.sort_values(by='Time')\n",
    "\n",
    "train_size = int(0.8 * len(df))\n",
    "\n",
    "train_df = df[:train_size]\n",
    "\n",
    "test_df = df[train_size:]\n",
    "\n",
    "def romove_users_pois_test(df_train, df_test):\n",
    "    users_train = df_train['Uid'].unique()\n",
    "    pois_train = df_train['Pid'].unique()\n",
    "    df_test = df_test[df_test['Uid'].isin(users_train)]\n",
    "    df_test = df_test[df_test['Pid'].isin(pois_train)]\n",
    "    return df_test\n",
    "\n",
    "test_df = romove_users_pois_test(train_df, test_df)\n",
    "\n",
    "\n",
    "test_uids = test_df['Uid'].unique()\n",
    "\n",
    "expanded_df = df[df['Uid'].isin(test_uids)]\n",
    "\n",
    "train_df.to_csv(f'datasets/{dataset}/train_data.csv', index=False)\n",
    "expanded_df.to_csv(f'datasets/{dataset}/test_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def generate_train_sequences(df: pd.DataFrame, window_size: int, step_size: int, mask_prob: float) -> pd.DataFrame:\n",
    "\n",
    "    df = df.copy()\n",
    "    df['Time'] = pd.to_datetime(df['Time'])\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for uid, group in df.groupby('Uid'):\n",
    "        group = group.sort_values('Time').reset_index(drop=True)\n",
    "        \n",
    "        if len(group) > 80:\n",
    "            group = group.iloc[-80:]\n",
    "        \n",
    "        n = len(group)\n",
    "\n",
    "        if n < window_size:\n",
    "            if n >= 10:  \n",
    "                input_pids = group['Pid'].iloc[:-1].tolist()\n",
    "                input_times = group['Time'].iloc[:-1].tolist()\n",
    "                target_pid = group['Pid'].iloc[-1]\n",
    "                target_time = group['Time'].iloc[-1]\n",
    "\n",
    "                results.append({\n",
    "                    'Uid': uid,\n",
    "                    'Pids': input_pids,\n",
    "                    'Times': input_times,\n",
    "                    'Target': target_pid,\n",
    "                    'Target_time': target_time\n",
    "                })\n",
    "            continue \n",
    "\n",
    "        for start in range(n - 1, window_size - 2, -step_size):\n",
    "            end = start + 1 \n",
    "            window = group.iloc[start - window_size + 1 : start + 1]\n",
    "\n",
    "            input_pids = window['Pid'].iloc[:-1].tolist()\n",
    "            input_times = window['Time'].iloc[:-1].tolist()\n",
    "            original_target_pid = window['Pid'].iloc[-1]\n",
    "            original_target_time = window['Time'].iloc[-1]\n",
    "\n",
    "            if random.random() < mask_prob and len(input_pids) >= 1:\n",
    "                drop_idx = random.randint(0, len(input_pids) - 1)\n",
    "                target_pid = input_pids[drop_idx]\n",
    "                target_time = input_times[drop_idx]\n",
    "                input_pids = input_pids[:drop_idx] + input_pids[drop_idx + 1:] + [original_target_pid]\n",
    "                input_times = input_times[:drop_idx] + input_times[drop_idx + 1:] + [original_target_time]\n",
    "            else:\n",
    "                target_pid = original_target_pid\n",
    "                target_time = original_target_time\n",
    "\n",
    "            results.append({\n",
    "                'Uid': uid,\n",
    "                'Pids': input_pids,\n",
    "                'Times': input_times,\n",
    "                'Target': target_pid,\n",
    "                'Target_time': target_time\n",
    "            })\n",
    "\n",
    "    train = pd.DataFrame(results)\n",
    "\n",
    "    train['Times'] = train['Times'].apply(lambda x: [t.strftime('%Y-%m-%d %H:%M') for t in x])\n",
    "    train['Target_time'] = train['Target_time'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "    return train\n",
    "\n",
    "def generate_test_sequences(test_df: pd.DataFrame, window_size: int):\n",
    "    \n",
    "    test_df = test_df.copy()\n",
    "    test_df['Time'] = pd.to_datetime(test_df['Time'])\n",
    "\n",
    "    val_records = []\n",
    "    test_records = []\n",
    "\n",
    "    for uid, group in test_df.groupby('Uid'):\n",
    "        group = group.sort_values('Time').reset_index(drop=True)\n",
    "        n = len(group)\n",
    "\n",
    "        if n < window_size:\n",
    "            if n > 2:\n",
    "                test_records.append({\n",
    "                    'Uid': uid,\n",
    "                    'Pids': group['Pid'].iloc[:-1].tolist(),\n",
    "                    'Times': group['Time'].iloc[:-1].tolist(),\n",
    "                    'Target': group['Pid'].iloc[-1],\n",
    "                    'Target_time': group['Time'].iloc[-1]\n",
    "                })\n",
    "                val_records.append({\n",
    "                    'Uid': uid,\n",
    "                    'Pids': group['Pid'].iloc[:-2].tolist(),\n",
    "                    'Times': group['Time'].iloc[:-2].tolist(),\n",
    "                    'Target': group['Pid'].iloc[-2],\n",
    "                    'Target_time': group['Time'].iloc[-2]\n",
    "                })\n",
    "            continue\n",
    "\n",
    "        if n >= window_size + 1:\n",
    "            val_start = n - window_size - 1\n",
    "            val_window = group.iloc[val_start:val_start + window_size]\n",
    "            val_records.append({\n",
    "                'Uid': uid,\n",
    "                'Pids': val_window['Pid'].iloc[:-1].tolist(),\n",
    "                'Times': val_window['Time'].iloc[:-1].tolist(),\n",
    "                'Target': val_window['Pid'].iloc[-1],\n",
    "                'Target_time': val_window['Time'].iloc[-1]\n",
    "            })\n",
    "\n",
    "        test_window = group.iloc[n - window_size:]\n",
    "        test_records.append({\n",
    "            'Uid': uid,\n",
    "            'Pids': test_window['Pid'].iloc[:-1].tolist(),\n",
    "            'Times': test_window['Time'].iloc[:-1].tolist(),\n",
    "            'Target': test_window['Pid'].iloc[-1],\n",
    "            'Target_time': test_window['Time'].iloc[-1]\n",
    "        })\n",
    "\n",
    "    val_df = pd.DataFrame(val_records)\n",
    "    test_df = pd.DataFrame(test_records)\n",
    "\n",
    "    for df in [val_df, test_df]:\n",
    "        df['Times'] = df['Times'].apply(lambda x: [t.strftime('%Y-%m-%d %H:%M') for t in x])\n",
    "        df['Target_time'] = df['Target_time'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "    return val_df, test_df\n",
    "\n",
    "\n",
    "\n",
    "dataset = \"NYC\"\n",
    "train = pd.read_csv(f\"datasets/{dataset}/train_data.csv\")\n",
    "test = pd.read_csv(f\"datasets/{dataset}/test_data.csv\")\n",
    "all_data = pd.read_csv(f\"datasets/{dataset}/data.csv\")\n",
    "\n",
    "train = generate_train_sequences(train, 50, 10, 0.1)\n",
    "val, test = generate_test_sequences(test, 50)\n",
    "\n",
    "val_all, test_all = generate_test_sequences(all_data, 50)\n",
    "test_all.to_csv(f\"datasets/{dataset}/data/test_all.csv\", index=False)\n",
    "\n",
    "train.to_csv(f\"datasets/{dataset}/data/train.csv\", index=False)\n",
    "val.to_csv(f\"datasets/{dataset}/data/val.csv\", index=False)\n",
    "test.to_csv(f\"datasets/{dataset}/data/test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = \"NYC\"\n",
    "file_name = f\"datasets/{dataset}/data.csv\"\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "df = df[['Uid', 'Pid', 'Time']]\n",
    "\n",
    "user_history_length = df.groupby('Uid').size()\n",
    "\n",
    "average_history_length = user_history_length.mean()\n",
    "\n",
    "print(average_history_length)\n",
    "\n",
    "history_length_counts = user_history_length.value_counts()\n",
    "\n",
    "most_frequent_length = history_length_counts.idxmax()\n",
    "\n",
    "print(most_frequent_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "POIRec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
